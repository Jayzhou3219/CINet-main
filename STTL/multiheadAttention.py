import copy

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import TransformerEncoder, TransformerEncoderLayer
from src.parser import get_parser


class MultiheadAttention(nn.Module):
    __constants__ = ['q_proj_weight', 'k_proj_weight', 'v_proj_weight', 'in_proj_weight']

    def __init__(self, additional_info_upper, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None,
                 vdim=None):
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.additional_info_upper = additional_info_upper
        # 如果没有单独为kdim,vdim设置维度，则设置成和 embed_dim 同样的值
        self.kdim = kdim if kdim is not None else embed_dim  # kdim:32
        self.vdim = vdim if vdim is not None else embed_dim  # vdim:32
        # 判断最后的值是否相同，设置一个标签
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads  # 既然是做多头注意力,那么每一个头分得的embedding就是总共的embedding维数除以头数
        assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"

        if self._qkv_same_embed_dim is False:
            self.q_proj_weight = nn.Parameter(torch.Tensor(embed_dim, embed_dim))
            self.k_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.kdim))
            self.v_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.vdim))
            self.register_parameter('in_proj_weight', None)
        else:
            # 通常，我们的参数都是一些常见的结构（卷积、全连接等）里面的计算参数。而当我们的网络有一些其他的设计时，会需要一些额外的参数同样-
            # 跟着整个网络的训练进行学习更新，最后得到最优的值，经典的例子有注意力机制中的权重参数、Vision Transformer中的class token和-
            # positional embedding等。
            # nn.Parameter():添加的参数会被添加到Parameters列表中，送入优化器中随训练一起学习更新,这么做是为了方便一次性初始化
            self.in_proj_weight = nn.Parameter(torch.empty(3 * embed_dim, embed_dim))  # torch.empty(size,out):创建一个未被初始化数值的tensor,tensor的大小是由size确定,out可选
            self.register_parameter('q_proj_weight', None)  # 设置输入的投影权重
            self.register_parameter('k_proj_weight', None)
            self.register_parameter('v_proj_weight', None)

        if bias:
            self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim))  # 初始化in_proj_bias
        else:
            self.register_parameter('in_proj_bias', None)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)  # 初始化out_proj

        if add_bias_kv:
            self.bias_k = nn.Parameter(torch.empty(1, 1, embed_dim))
            self.bias_v = nn.Parameter(torch.empty(1, 1, embed_dim))
        else:
            self.bias_k = self.bias_v = None

        self.add_zero_attn = add_zero_attn  # 如果设为True,则会在内部运算中在k、v的序列长度纬度增加一列零向量

        self._reset_parameters()

    # 这是pytorch 中专用的初始化参数的方法
    def _reset_parameters(self):
        if self._qkv_same_embed_dim:
            nn.init.xavier_uniform_(self.in_proj_weight)  # nn.init.xavier_uniform_(tensor,gain=1.0):Glorot初始化,使用均匀分布值填充输入Tensor
        else:
            nn.init.xavier_uniform_(self.q_proj_weight)
            nn.init.xavier_uniform_(self.k_proj_weight)
            nn.init.xavier_uniform_(self.v_proj_weight)

        if self.in_proj_bias is not None:
            nn.init.constant_(self.in_proj_bias, 0.)  # nn.init.constant_(tensor,val):用val的值填充输入的张量或变量
            nn.init.constant_(self.out_proj.bias, 0.)
        if self.bias_k is not None:
            nn.init.xavier_normal_(self.bias_k)
        if self.bias_v is not None:
            nn.init.xavier_normal_(self.bias_v)

    def __setstate__(self, state):
        # Support loading old MultiheadAttention checkpoints generated by v1.1.0
        if '_qkv_same_embed_dim' not in state:
            state['_qkv_same_embed_dim'] = True

        super(MultiheadAttention, self).__setstate__(state)

    def forward(self, query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None):
        if not self._qkv_same_embed_dim:
            return multi_head_attention_forward(
                query, key, value, self.embed_dim, self.num_heads,
                self.in_proj_weight, self.in_proj_bias,
                self.bias_k, self.bias_v, self.add_zero_attn,
                self.dropout, self.out_proj.weight, self.out_proj.bias,
                training=self.training,
                key_padding_mask=key_padding_mask, need_weights=need_weights,
                attn_mask=attn_mask, use_separate_proj_weight=True,
                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,
                v_proj_weight=self.v_proj_weight)
        else:
            return multi_head_attention_forward(self.additional_info_upper, device='cuda:1')

class multi_head_attention_forward(nn.Module):
    def __init__(self, inputs, device):
        super().__init__()
        self.device = self._set_device()

        ##################
        # MODEL PARAMETERS
        ##################

        # Set parameters for network architechure
        self.input_size = 2  # size of the input 2: (x,y)
        self.embedding_size = 32  # embedding dimension
        self.nhead = 8  # number of heads in multi-head attentions TF(transformer)
        self.d_hidden = 2048  # hidden dimension in the TF encoder layer
        self.n_layers_temporal = 1  # number of TransformerEncoderLayers
        self.dropout_prob = 0  # the dropout probability value
        # self.add_noise_traj = self.args.add_noise_traj
        self.noise_size = 16  # size of random noise vector
        self.output_size = 2  # output size
        self.dropout_TF_prob = 0.1  # dropout in transformer encoder layer

        # Goal module parameters
        self.num_image_channels = 6

        # Extra information to concat goals: time, last position, prediction final positions,and distance to predicted goals
        self.extra_features = 4

        #######################
        # Upper branch of Model
        #######################

        # Temporal Transformer layer to extract temporal information of pedestrians
        self.temporal_encoder_layer = TransformerEncoderLayer(
            d_model=self.embedding_size + self.extra_features * self.nhead,
            nhead=self.nhead,
            dim_feedforward=self.d_hidden,
            dropout=self.dropout_TF_prob)
        self.temporal_encoder = TransformerEncoder(self.temporal_encoder_layer, num_layers=self.n_layers_temporal).to(self.device)
        # Spatial Transformer layer to extract spatial information of pedestrians

        self.spatial_encoder_layer = TransformerEncoderLayer(
            d_model=self.embedding_size + self.extra_features * self.nhead,
            nhead=self.nhead,
            dim_feedforward=self.d_hidden,
            dropout=self.dropout_TF_prob)
        self.spatial_encoder = TransformerEncoder(self.spatial_encoder_layer, num_layers=self.n_layers_temporal).to(self.device)

        # Cross attention
        self.cross_atten = nn.MultiheadAttention(embed_dim=64, num_heads=self.nhead, dropout=self.dropout_TF_prob).to(self.device)

    def forward(self, inputs):
        # Excellent!
        temporal_output_1 = self.temporal_encoder(inputs)  # (8,32,64)

        spatial_input_embedded_ = inputs.permute(1, 0, 2)
        spatial_output_2 = self.spatial_encoder(spatial_input_embedded_)

        attn_output_up, _ = self.cross_atten(
            spatial_output_2.permute(1, 0, 2),
            temporal_output_1,
            temporal_output_1)  # (8,32,64)
        attn_output_up = attn_output_up.to(self.device)
        _ = _.to(self.device)

        return attn_output_up, _

    def _set_device(self):
        """
        Set the device for the experiment. GPU if available, else CPU.
        """
        # torch.cuda.is_available() already checked in parsed args
        device = torch.device('cuda:1')
        # print('\nUsing device:', device)
        #
        # # Additional info when using cuda
        # if device.type == 'cuda':
        #     print('Number of available GPUs:', torch.cuda.device_count())
        #     print('GPU name:', torch.cuda.get_device_name(0))
        #     print('Cuda version:', torch.version.cuda)
        # print()
        return device












